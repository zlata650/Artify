#!/usr/bin/env python3
"""
Module de validation et nettoyage des URLs pour le scraping d'√©v√©nements.
Garantit que les liens m√®nent √† des pages d'√©v√©nements avec possibilit√© de r√©servation.
"""

import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import time

# Headers pour les requ√™tes
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
}

# Patterns d'URL qui ne sont PAS des pages d'√©v√©nements individuels
EXCLUDE_URL_PATTERNS = [
    # Pages de listes/cat√©gories
    r'/guides?/',
    r'/categories?/',
    r'/rubrique/',
    r'/tag/',
    r'/tags/',
    r'/search',
    r'/recherche',
    r'\?page=\d+',
    r'/page/\d+',
    r'/p/\d+',
    r'/index\.html?$',
    r'/accueil',
    r'/home',
    
    # Pages d'aide/l√©gales
    r'/mentions-legales',
    r'/cgu',
    r'/cgv',
    r'/politique-confidentialite',
    r'/privacy',
    r'/terms',
    r'/contact',
    r'/about',
    r'/a-propos',
    r'/faq',
    r'/aide',
    r'/help',
    
    # Pages compte utilisateur
    r'/login',
    r'/connexion',
    r'/inscription',
    r'/register',
    r'/signup',
    r'/mon-compte',
    r'/account',
    r'/profil',
    r'/panier',
    r'/cart',
    r'/checkout',
    
    # R√©seaux sociaux
    r'facebook\.com',
    r'twitter\.com',
    r'instagram\.com',
    r'linkedin\.com',
    r'youtube\.com',
    r'tiktok\.com',
    
    # Pages d'organisateurs/lieux (pas d'√©v√©nements sp√©cifiques)
    r'/organizer',
    r'/organiser',
    r'/organisateur',
    r'/organisateurs',
    r'/profile/',
    r'/profil/',
    r'/venue/',
    r'/lieu/',
    r'/lieux/',
    r'/salle/',
    r'/salles/',
    r'/artiste/',
    r'/artist/',
    r'/artists/',
    r'/artistes/',
    r'/band/',
    r'/groupe/',
    
    # Autres
    r'/newsletter',
    r'/rss',
    r'/feed',
    r'/sitemap',
    r'/plan-du-site',
    r'\.pdf$',
    r'\.jpg$',
    r'\.png$',
    r'\.gif$',
    r'#',  # Ancres
]

# Patterns d'URL d'organisateurs √† √©viter (pages racine de sites)
ORGANIZER_URL_PATTERNS = [
    # Ces URLs sont des pages d'accueil de lieux/organisateurs, pas d'√©v√©nements
    r'^https?://[^/]+/?$',  # Juste le domaine sans chemin
    r'^https?://[^/]+/fr/?$',  # Page d'accueil en fran√ßais
    r'^https?://[^/]+/en/?$',  # Page d'accueil en anglais
    r'^https?://www\.[^/]+/?$',  # www.site.com/
]

# Patterns d'URL qui indiquent une page d'√©v√©nement INDIVIDUEL
EVENT_URL_PATTERNS = {
    'sortiraparis.com': [
        r'/articles/\d+',  # Articles individuels
        r'/[^/]+/[^/]+/articles/\d+-',  # Ex: /scenes/concert-musique/articles/12345-nom
    ],
    'allocine.fr': [
        r'/film/fichefilm_gen_cfilm=\d+',  # Fiche film
        r'/seance/salle_gen_csalle=\d+',  # S√©ance
    ],
    'premiere.fr': [
        r'/film/[^/]+-\d+',  # Film avec ID
    ],
    'telerama.fr': [
        r'/cinema/films?/[^/]+',  # Film individuel
    ],
    'fnacspectacles.com': [
        r'/place-spectacle/manifestation/',  # Page √©v√©nement
        r'/[^/]+-a\d+',  # Event avec ID
    ],
    'ticketmaster.fr': [
        r'/event/\d+',
        r'/artist/\d+',
        r'/manifestation/',
    ],
    'billetreduc.com': [
        r'/\d+/',  # ID num√©rique
    ],
    'legrandrex.com': [
        r'/films?/[^/]+$',  # Fiche film sp√©cifique
    ],
    'cinemalouxor.fr': [
        r'/films?/[^/]+$',
    ],
    'mk2.com': [
        r'/films?/[^/]+$',
    ],
    'operadeparis.fr': [
        r'/saison-\d+/[^/]+$',  # Page spectacle sp√©cifique
        r'/spectacles/[^/]+$',
    ],
    'philharmoniedeparis.fr': [
        r'/concert/\d+',  # Concert sp√©cifique
        r'/activite/\d+',
    ],
    'theatredelaville-paris.com': [
        r'/spectacle/[^/]+$',
    ],
    'olympiahall.com': [
        r'/concert/[^/]+$',
        r'/spectacle/[^/]+$',
    ],
    'zenith-paris.com': [
        r'/programmation/[^/]+$',
    ],
}

# Mots-cl√©s indiquant une possibilit√© de r√©servation/achat
BOOKING_KEYWORDS = [
    'r√©server', 'reserver', 'reservation', 'r√©servation',
    'acheter', 'achat', 'acheter-billets', 'buy-tickets',
    'billets', 'billet', 'tickets', 'ticket',
    'places', 'place', 'entr√©es', 'entree', 'entr√©e',
    'tarif', 'tarifs', 'prix',
    'prochaines-dates', 'dates', 's√©ances', 'seances', 'horaires',
    'disponible', 'disponibles',
    'ajouter-au-panier', 'add-to-cart',
    'booking', 'book-now',
]

# Mots-cl√©s qui indiquent que ce n'est PAS un √©v√©nement
NON_EVENT_KEYWORDS = [
    'tous les concerts', 'tous les √©v√©nements', 'toutes les dates',
    'voir tout', 'voir plus', 'afficher plus',
    'liste des', 'agenda', 'calendrier',
    'nos partenaires', 'nos sponsors',
    '√† propos', 'qui sommes-nous',
    'conditions g√©n√©rales', 'mentions l√©gales',
]


def is_organizer_url(url: str) -> bool:
    """
    V√©rifie si l'URL est une page d'organisateur/lieu (pas un √©v√©nement sp√©cifique).
    
    Args:
        url: L'URL √† v√©rifier
        
    Returns:
        True si c'est une page d'organisateur, False sinon
    """
    if not url:
        return True
    
    parsed = urlparse(url)
    path = parsed.path.strip('/')
    
    # URL sans chemin = page d'accueil = page d'organisateur
    if not path or path in ['', 'fr', 'en', 'de', 'es', 'it']:
        return True
    
    # V√©rifier les patterns d'organisateurs
    for pattern in ORGANIZER_URL_PATTERNS:
        if re.search(pattern, url, re.IGNORECASE):
            return True
    
    # Chemin tr√®s court sans ID = probablement page de liste
    path_parts = [p for p in path.split('/') if p]
    if len(path_parts) == 1:
        # Un seul segment comme /concerts, /events, /spectacles = page liste
        single_segment_lists = [
            'concerts', 'events', 'evenements', 'spectacles', 'programmation',
            'agenda', 'billetterie', 'tickets', 'programme', 'saison',
            'films', 'seances', 'cinema', 'expositions', 'exhibitions'
        ]
        if path_parts[0].lower() in single_segment_lists:
            return True
    
    return False


def is_valid_event_url_pattern(url: str) -> bool:
    """
    V√©rifie si l'URL correspond au pattern d'une page d'√©v√©nement individuel.
    
    Args:
        url: L'URL √† v√©rifier
        
    Returns:
        True si l'URL semble √™tre une page d'√©v√©nement, False sinon
    """
    # D'abord v√©rifier si c'est une URL d'organisateur
    if is_organizer_url(url):
        return False
    
    # Ensuite, exclure les patterns non-√©v√©nements
    for pattern in EXCLUDE_URL_PATTERNS:
        if re.search(pattern, url, re.IGNORECASE):
            return False
    
    # Ensuite, v√©rifier si c'est un pattern d'√©v√©nement connu
    parsed = urlparse(url)
    domain = parsed.netloc.replace('www.', '')
    
    for site_domain, patterns in EVENT_URL_PATTERNS.items():
        if site_domain in domain:
            for pattern in patterns:
                if re.search(pattern, url, re.IGNORECASE):
                    return True
            # Si c'est un domaine connu mais ne matche pas les patterns, c'est suspect
            return False
    
    # Pour les domaines inconnus, on v√©rifie quelques heuristiques
    path = parsed.path.lower()
    
    # Un article individuel a g√©n√©ralement un ID num√©rique ou un slug unique
    if re.search(r'/\d{4,}[-/]', url):  # ID num√©rique d'au moins 4 chiffres
        return True
    
    if re.search(r'/articles?/\d+', url):  # /article/123 ou /articles/123
        return True
    
    # URLs avec /event/ ou /spectacle/ + slug = √©v√©nement individuel
    if re.search(r'/(event|events|spectacle|spectacles|concert|concerts|seance|film)/[^/]+', url, re.IGNORECASE):
        return True
        
    # √âviter les pages de liste courtes (/, /concerts, /events)
    path_parts = [p for p in path.split('/') if p]
    if len(path_parts) < 2:
        return False
    
    return True


def clean_url(url: str, base_url: str = None) -> str:
    """
    Nettoie et normalise une URL.
    
    Args:
        url: L'URL √† nettoyer
        base_url: URL de base pour les liens relatifs
        
    Returns:
        URL nettoy√©e ou None si invalide
    """
    if not url or not isinstance(url, str):
        return None
    
    url = url.strip()
    
    # Ignorer les ancres, javascript, mailto
    if url.startswith(('#', 'javascript:', 'mailto:', 'tel:')):
        return None
    
    # Convertir les liens relatifs en absolus
    if url.startswith('/'):
        if base_url:
            url = urljoin(base_url, url)
        else:
            return None  # Impossible de r√©soudre sans base_url
    
    # V√©rifier que c'est une URL HTTP(S) valide
    if not url.startswith(('http://', 'https://')):
        if base_url:
            url = urljoin(base_url, url)
        else:
            return None
    
    # Supprimer les fragments d'ancre (#...)
    url = url.split('#')[0]
    
    # Supprimer les param√®tres de tracking courants
    tracking_params = ['utm_source', 'utm_medium', 'utm_campaign', 'utm_term', 'utm_content', 
                       'fbclid', 'gclid', 'ref', 'source']
    parsed = urlparse(url)
    
    if parsed.query:
        params = parsed.query.split('&')
        clean_params = [p for p in params if not any(tp in p.lower() for tp in tracking_params)]
        if clean_params:
            url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}?{'&'.join(clean_params)}"
        else:
            url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
    
    return url


def has_booking_indicators(html_content: str) -> bool:
    """
    V√©rifie si la page contient des indicateurs de r√©servation/achat de billets.
    
    Args:
        html_content: Le contenu HTML de la page
        
    Returns:
        True si des indicateurs de r√©servation sont trouv√©s
    """
    if not html_content:
        return False
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # Chercher dans les boutons et liens
    for elem in soup.find_all(['a', 'button', 'input']):
        text = elem.get_text(strip=True).lower()
        href = elem.get('href', '').lower()
        class_attr = ' '.join(elem.get('class', [])).lower()
        
        for keyword in BOOKING_KEYWORDS:
            if keyword in text or keyword in href or keyword in class_attr:
                return True
    
    # Chercher dans les textes en g√©n√©ral
    page_text = soup.get_text(separator=' ').lower()
    
    # V√©rifier les mots-cl√©s de r√©servation
    booking_found = sum(1 for kw in BOOKING_KEYWORDS if kw in page_text) >= 2
    
    # V√©rifier les indicateurs n√©gatifs
    non_event_found = any(kw in page_text for kw in NON_EVENT_KEYWORDS)
    
    if non_event_found and not booking_found:
        return False
    
    return booking_found


def validate_event_url(url: str, verify_booking: bool = False, timeout: int = 10) -> dict:
    """
    Valide compl√®tement une URL d'√©v√©nement.
    
    Args:
        url: L'URL √† valider
        verify_booking: Si True, v√©rifie que la page a des options de r√©servation
        timeout: Timeout pour les requ√™tes HTTP
        
    Returns:
        Dict avec les r√©sultats de validation:
        {
            'valid': bool,
            'url': str (URL nettoy√©e),
            'reason': str (raison si invalide),
            'has_booking': bool (si verify_booking=True)
        }
    """
    result = {
        'valid': False,
        'url': url,
        'reason': None,
        'has_booking': None
    }
    
    # Nettoyer l'URL
    cleaned_url = clean_url(url)
    if not cleaned_url:
        result['reason'] = "URL invalide ou non nettoyable"
        return result
    
    result['url'] = cleaned_url
    
    # V√©rifier le pattern de l'URL
    if not is_valid_event_url_pattern(cleaned_url):
        result['reason'] = "Pattern d'URL ne correspond pas √† un √©v√©nement"
        return result
    
    # Si on veut v√©rifier la capacit√© de r√©servation
    if verify_booking:
        try:
            response = requests.get(cleaned_url, headers=HEADERS, timeout=timeout)
            
            if response.status_code != 200:
                result['reason'] = f"Page inaccessible (HTTP {response.status_code})"
                return result
            
            has_booking = has_booking_indicators(response.text)
            result['has_booking'] = has_booking
            
            if not has_booking:
                result['reason'] = "Pas d'options de r√©servation d√©tect√©es"
                return result
                
        except requests.RequestException as e:
            result['reason'] = f"Erreur de connexion: {str(e)}"
            return result
    
    result['valid'] = True
    return result


def filter_event_urls(urls: list, base_url: str = None, verify_booking: bool = False, 
                      max_verify: int = 50, verbose: bool = False) -> list:
    """
    Filtre une liste d'URLs pour ne garder que celles menant √† des √©v√©nements.
    
    Args:
        urls: Liste d'URLs √† filtrer
        base_url: URL de base pour les liens relatifs
        verify_booking: Si True, v√©rifie les pages pour la r√©servation (plus lent)
        max_verify: Nombre maximum d'URLs √† v√©rifier en d√©tail
        verbose: Si True, affiche les d√©tails du filtrage
        
    Returns:
        Liste d'URLs valides
    """
    valid_urls = []
    verified_count = 0
    
    for url in urls:
        # Nettoyer l'URL
        cleaned = clean_url(url, base_url)
        if not cleaned:
            if verbose:
                print(f"  ‚úó URL invalide: {url[:50]}...")
            continue
        
        # V√©rifier le pattern
        if not is_valid_event_url_pattern(cleaned):
            if verbose:
                print(f"  ‚úó Pattern non-√©v√©nement: {cleaned[:50]}...")
            continue
        
        # V√©rification approfondie (optionnelle, limit√©e)
        if verify_booking and verified_count < max_verify:
            result = validate_event_url(cleaned, verify_booking=True)
            verified_count += 1
            
            if not result['valid']:
                if verbose:
                    print(f"  ‚úó {result['reason']}: {cleaned[:50]}...")
                continue
            
            if verbose:
                print(f"  ‚úì √âv√©nement valid√©: {cleaned[:50]}...")
            
            # Petit d√©lai pour √©viter de surcharger le serveur
            time.sleep(0.3)
        
        valid_urls.append(cleaned)
    
    return list(set(valid_urls))  # Supprimer les doublons


def validate_and_clean_events(events: list, base_url: str = None, 
                               url_key: str = 'url', verbose: bool = False) -> list:
    """
    Valide et nettoie une liste d'√©v√©nements (dictionnaires).
    
    Args:
        events: Liste de dictionnaires d'√©v√©nements
        base_url: URL de base pour les liens relatifs
        url_key: Cl√© du dictionnaire contenant l'URL
        verbose: Si True, affiche les d√©tails
        
    Returns:
        Liste d'√©v√©nements avec URLs valid√©es
    """
    valid_events = []
    
    for event in events:
        url = event.get(url_key)
        
        if not url:
            continue
        
        # Nettoyer l'URL
        cleaned = clean_url(url, base_url)
        if not cleaned:
            if verbose:
                print(f"  ‚úó URL invalide: {url[:50] if url else 'None'}...")
            continue
        
        # V√©rifier le pattern
        if not is_valid_event_url_pattern(cleaned):
            if verbose:
                print(f"  ‚úó Non-√©v√©nement: {cleaned[:50]}...")
            continue
        
        # Mettre √† jour l'URL nettoy√©e
        event[url_key] = cleaned
        valid_events.append(event)
    
    if verbose:
        print(f"\n  ‚Üí {len(valid_events)}/{len(events)} √©v√©nements valid√©s")
    
    return valid_events


# Fonctions sp√©cifiques par source
def is_sortiraparis_event_url(url: str) -> bool:
    """V√©rifie si c'est une URL d'√©v√©nement SortiraParis valide."""
    if 'sortiraparis.com' not in url:
        return False
    
    # Les articles d'√©v√©nements ont le format /xxx/xxx/articles/ID-titre
    if '/articles/' in url and re.search(r'/articles/\d+-', url):
        return True
    
    return False


def is_allocine_event_url(url: str) -> bool:
    """V√©rifie si c'est une URL de film AlloCin√© valide."""
    if 'allocine.fr' not in url:
        return False
    
    # Les fiches films ont le format /film/fichefilm_gen_cfilm=ID.html
    if '/film/fichefilm_gen_cfilm=' in url:
        return True
    
    return False


def verify_event_page_with_booking(url: str, timeout: int = 10) -> dict:
    """
    V√©rifie si une URL m√®ne √† une page d'√©v√©nement avec options de r√©servation.
    
    Args:
        url: L'URL √† v√©rifier
        timeout: Timeout pour la requ√™te HTTP
        
    Returns:
        Dict avec les r√©sultats:
        {
            'is_event': bool,
            'has_booking': bool,
            'title': str or None,
            'error': str or None
        }
    """
    result = {
        'is_event': False,
        'has_booking': False,
        'title': None,
        'error': None
    }
    
    try:
        response = requests.get(url, headers=HEADERS, timeout=timeout)
        
        if response.status_code != 200:
            result['error'] = f"HTTP {response.status_code}"
            return result
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Extraire le titre
        title_tag = soup.find('title') or soup.find('h1')
        if title_tag:
            result['title'] = title_tag.get_text(strip=True)[:100]
        
        # V√©rifier si c'est une page d'√©v√©nement (pr√©sence de date, lieu, etc.)
        page_text = soup.get_text(separator=' ').lower()
        
        event_indicators = ['date', 'lieu', 'horaire', 'heure', 'adresse', 
                           's√©ance', 'tarif', 'prix', 'salle', 'dur√©e']
        event_count = sum(1 for ind in event_indicators if ind in page_text)
        result['is_event'] = event_count >= 3
        
        # V√©rifier la capacit√© de r√©servation
        result['has_booking'] = has_booking_indicators(response.text)
        
    except requests.RequestException as e:
        result['error'] = str(e)
    
    return result


def batch_validate_urls(urls: list, verify_booking: bool = False, 
                        max_concurrent: int = 5, verbose: bool = True) -> list:
    """
    Valide un lot d'URLs et retourne uniquement les valides.
    
    Args:
        urls: Liste d'URLs √† valider
        verify_booking: Si True, v√©rifie la pr√©sence d'options de r√©servation
        max_concurrent: Nombre max d'URLs √† v√©rifier en d√©tail
        verbose: Si True, affiche les statistiques
        
    Returns:
        Liste d'URLs valid√©es
    """
    valid_urls = []
    stats = {
        'total': len(urls),
        'valid_pattern': 0,
        'invalid_pattern': 0,
        'has_booking': 0,
        'no_booking': 0,
        'errors': 0
    }
    
    for i, url in enumerate(urls):
        # V√©rification du pattern
        cleaned = clean_url(url)
        if not cleaned or not is_valid_event_url_pattern(cleaned):
            stats['invalid_pattern'] += 1
            continue
        
        stats['valid_pattern'] += 1
        
        # V√©rification approfondie optionnelle
        if verify_booking and i < max_concurrent:
            result = verify_event_page_with_booking(cleaned)
            
            if result['error']:
                stats['errors'] += 1
                continue
            
            if result['has_booking']:
                stats['has_booking'] += 1
                valid_urls.append(cleaned)
            else:
                stats['no_booking'] += 1
            
            time.sleep(0.3)
        else:
            valid_urls.append(cleaned)
    
    if verbose:
        print(f"\nüìä Statistiques de validation:")
        print(f"   Total: {stats['total']} URLs")
        print(f"   ‚úì Pattern valide: {stats['valid_pattern']}")
        print(f"   ‚úó Pattern invalide: {stats['invalid_pattern']}")
        if verify_booking:
            print(f"   ‚úì Avec r√©servation: {stats['has_booking']}")
            print(f"   ‚úó Sans r√©servation: {stats['no_booking']}")
            print(f"   ‚ö† Erreurs: {stats['errors']}")
        print(f"   ‚Üí URLs retenues: {len(valid_urls)}")
    
    return valid_urls


def log_url_validation(url: str, result: bool, reason: str = None):
    """
    Affiche le r√©sultat de la validation d'une URL.
    """
    status = "‚úì" if result else "‚úó"
    print(f"{status} {url[:70]}{'...' if len(url) > 70 else ''}")
    if reason:
        print(f"  ‚Üí {reason}")


def test_event_urls_from_list(urls: list, description: str = "Test URLs"):
    """
    Teste une liste d'URLs et affiche les statistiques.
    """
    print(f"\n{'='*60}")
    print(f"üìã {description}")
    print(f"{'='*60}\n")
    
    valid_count = 0
    organizer_count = 0
    invalid_count = 0
    
    for url in urls:
        cleaned = clean_url(url)
        
        if not cleaned:
            log_url_validation(url, False, "URL invalide ou non nettoyable")
            invalid_count += 1
            continue
        
        if is_organizer_url(cleaned):
            log_url_validation(url, False, "Page d'organisateur/lieu (pas un √©v√©nement)")
            organizer_count += 1
            continue
        
        if is_valid_event_url_pattern(cleaned):
            log_url_validation(url, True, "URL d'√©v√©nement valide")
            valid_count += 1
        else:
            log_url_validation(url, False, "Pattern non reconnu comme √©v√©nement")
            invalid_count += 1
    
    print(f"\n{'='*60}")
    print(f"üìä R√©sultats:")
    print(f"   ‚úì √âv√©nements valides: {valid_count}")
    print(f"   ‚ö† Pages organisateurs: {organizer_count}")
    print(f"   ‚úó URLs invalides: {invalid_count}")
    print(f"   Total: {len(urls)} URLs")
    print(f"{'='*60}\n")
    
    return valid_count, organizer_count, invalid_count


if __name__ == "__main__":
    # Tests
    print("üîç Test du validateur d'URLs\n")
    
    # Test 1: URLs d'√©v√©nements valides
    valid_event_urls = [
        "https://www.sortiraparis.com/scenes/concert-musique/articles/123456-concert-test",
        "https://www.allocine.fr/film/fichefilm_gen_cfilm=123456.html",
        "https://www.sortiraparis.com/soiree/articles/98765-soiree-techno",
        "https://www.fnacspectacles.com/place-spectacle/manifestation/Concert-ORELSAN-ORE24.htm",
        "https://www.ticketmaster.fr/event/12345-concert-metallica",
    ]
    
    # Test 2: URLs de pages d'organisateurs (doivent √™tre rejet√©es)
    organizer_urls = [
        "https://www.sunset-sunside.com/",
        "https://philharmoniedeparis.fr/fr",
        "https://www.operadeparis.fr/",
        "https://comedie-francaise.fr",
        "https://olympiahall.com",
        "https://rexclub.com",
        "https://www.fondationlouisvuitton.fr/",
        "https://www.louvre.fr/",
        "https://le-zenith.com/",
    ]
    
    # Test 3: URLs invalides (pages de liste, contact, etc.)
    invalid_urls = [
        "https://www.sortiraparis.com/scenes/concert-musique",  # Page liste
        "https://www.sortiraparis.com/guides/concerts",  # Guide
        "https://www.sortiraparis.com/",  # Page d'accueil
        "https://facebook.com/events/123",  # R√©seau social
        "/contact",  # Page contact
        "#anchor",  # Ancre
        "javascript:void(0)",  # JavaScript
        "https://venue.com/organizer/123",  # Page organisateur
    ]
    
    # Ex√©cuter les tests
    test_event_urls_from_list(valid_event_urls, "URLs d'√âV√âNEMENTS valides (doivent passer)")
    test_event_urls_from_list(organizer_urls, "URLs d'ORGANISATEURS (doivent √™tre rejet√©es)")
    test_event_urls_from_list(invalid_urls, "URLs INVALIDES (doivent √™tre rejet√©es)")

