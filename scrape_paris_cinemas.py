#!/usr/bin/env python3
"""
Script pour scraper les affiches des cin√©mas de Paris.
Collecte les films √† l'affiche depuis plusieurs sources.
"""

import requests
from bs4 import BeautifulSoup
import sqlite3
from datetime import datetime
import time
import re
import warnings
from url_validator import clean_url, is_valid_event_url_pattern, is_allocine_event_url, validate_and_clean_events

warnings.filterwarnings('ignore')

# Liste compl√®te des cin√©mas de Paris
PARIS_CINEMAS = {
    # R√©seaux / Cha√Ænes
    "reseaux": [
        # UGC
        {"nom": "UGC Cin√© Cit√© Les Halles", "adresse": "7 place de la Rotonde, 75001 Paris", "type": "reseau"},
        {"nom": "UGC Cin√© Cit√© Bercy", "adresse": "2 cour Saint-√âmilion, 75012 Paris", "type": "reseau"},
        {"nom": "UGC Cin√© Cit√© Paris 19", "adresse": "166 boulevard Macdonald, 75019 Paris", "type": "reseau"},
        {"nom": "UGC Danton", "adresse": "99 boulevard Saint-Germain, 75006 Paris", "type": "reseau"},
        {"nom": "UGC Gobelins", "adresse": "66 avenue des Gobelins, 75013 Paris", "type": "reseau"},
        {"nom": "UGC Od√©on", "adresse": "124 boulevard Saint-Germain, 75006 Paris", "type": "reseau"},
        {"nom": "UGC Op√©ra", "adresse": "32 boulevard des Italiens, 75009 Paris", "type": "reseau"},
        {"nom": "UGC Normandie", "adresse": "116 avenue des Champs-√âlys√©es, 75008 Paris", "type": "reseau"},
        {"nom": "UGC Rotonde", "adresse": "103 boulevard du Montparnasse, 75006 Paris", "type": "reseau"},
        {"nom": "UGC Montparnasse", "adresse": "83 boulevard du Montparnasse, 75006 Paris", "type": "reseau"},
        {"nom": "UGC Lyon Bastille", "adresse": "12 rue de Lyon, 75012 Paris", "type": "reseau"},
        {"nom": "UGC Maillot", "adresse": "15 avenue de la Grande Arm√©e, 75016 Paris", "type": "reseau"},
        
        # Path√© / Gaumont
        {"nom": "Path√© Beaugrenelle", "adresse": "7 rue Linois, 75015 Paris", "type": "reseau"},
        {"nom": "Path√© Parnasse", "adresse": "67 boulevard du Montparnasse, 75006 Paris", "type": "reseau"},
        {"nom": "Path√© Wepler", "adresse": "140 boulevard de Clichy, 75018 Paris", "type": "reseau"},
        {"nom": "Path√© Convention", "adresse": "27 rue Alain Chartier, 75015 Paris", "type": "reseau"},
        {"nom": "Path√© Les Fauvettes", "adresse": "58 avenue des Gobelins, 75013 Paris", "type": "reseau"},
        {"nom": "Path√© Al√©sia", "adresse": "73 avenue du G√©n√©ral Leclerc, 75014 Paris", "type": "reseau"},
        {"nom": "Path√© La Villette", "adresse": "30 avenue Corentin Cariou, 75019 Paris", "type": "reseau"},
        {"nom": "Path√© Op√©ra Premier", "adresse": "2 rue Scribe, 75009 Paris", "type": "reseau"},
        {"nom": "Path√© Palace", "adresse": "2 rue Scribe, 75009 Paris", "type": "reseau"},
        {"nom": "Path√© Montparnos", "adresse": "31 rue du D√©part, 75014 Paris", "type": "reseau"},
        {"nom": "Path√© Aquaboulevard", "adresse": "8 rue du Colonel Pierre Avia, 75015 Paris", "type": "reseau"},
        {"nom": "Path√© La G√©ode", "adresse": "26 avenue Corentin Cariou, 75019 Paris", "type": "reseau", "url": "https://www.pathe.fr/cinemas/cinema-la-geode"},
        
        # MK2
        {"nom": "MK2 Biblioth√®que", "adresse": "128-162 avenue de France, 75013 Paris", "type": "reseau"},
        {"nom": "MK2 Biblioth√®que x Centre Pompidou", "adresse": "128-162 avenue de France, 75013 Paris", "type": "reseau"},
        {"nom": "MK2 Beaubourg", "adresse": "50 rue Rambuteau, 75003 Paris", "type": "reseau"},
        {"nom": "MK2 Bastille (c√¥t√© Beaumarchais)", "adresse": "4 boulevard Beaumarchais, 75011 Paris", "type": "reseau"},
        {"nom": "MK2 Bastille (c√¥t√© Faubourg)", "adresse": "37 rue du Faubourg Saint-Antoine, 75011 Paris", "type": "reseau"},
        {"nom": "MK2 Nation", "adresse": "133 boulevard Diderot, 75012 Paris", "type": "reseau"},
        {"nom": "MK2 Parnasse", "adresse": "11 rue Jules Chaplain, 75006 Paris", "type": "reseau"},
        {"nom": "MK2 Gambetta", "adresse": "6 rue Belgrand, 75020 Paris", "type": "reseau"},
        {"nom": "MK2 Quai de Loire", "adresse": "7 quai de Loire, 75019 Paris", "type": "reseau"},
        {"nom": "MK2 Quai de Seine", "adresse": "14 quai de la Seine, 75019 Paris", "type": "reseau"},
        {"nom": "MK2 Od√©on (c√¥t√© Saint-Germain)", "adresse": "113 boulevard Saint-Germain, 75006 Paris", "type": "reseau"},
        {"nom": "MK2 Od√©on (c√¥t√© Saint-Michel)", "adresse": "7 rue Hautefeuille, 75006 Paris", "type": "reseau"},
        
        # CGR
        {"nom": "CGR Paris Lilas", "adresse": "71 rue de Paris, 93260 Les Lilas", "type": "reseau", "url": "https://www.cgrcinemas.fr"},
    ],
    
    # Cin√©mas ind√©pendants et Art et Essai
    "independants": [
        # Grands ind√©pendants
        {"nom": "Le Grand Rex", "adresse": "1 boulevard Poissonni√®re, 75002 Paris", "type": "independant", "url": "https://www.legrandrex.com"},
        {"nom": "Max Linder Panorama", "adresse": "24 boulevard Poissonni√®re, 75009 Paris", "type": "independant", "url": "https://www.maxlinder.com"},
        {"nom": "Le Publicis Cin√©mas", "adresse": "133 avenue des Champs-√âlys√©es, 75008 Paris", "type": "independant"},
        
        # Centre / Historique
        {"nom": "Jeu de Paume", "adresse": "1 place de la Concorde, 75008 Paris", "type": "art_essai", "url": "https://www.jeudepaume.org"},
        {"nom": "Luminor H√¥tel de Ville", "adresse": "20 rue du Temple, 75004 Paris", "type": "art_essai"},
        {"nom": "Le Latina", "adresse": "20 rue du Temple, 75004 Paris", "type": "art_essai"},
        
        # Quartier Latin (5e arrondissement)
        {"nom": "Le Champo", "adresse": "51 rue des √âcoles, 75005 Paris", "type": "art_essai", "url": "https://www.lechampo.com"},
        {"nom": "Cin√©ma du Panth√©on", "adresse": "13 rue Victor Cousin, 75005 Paris", "type": "art_essai"},
        {"nom": "√âcoles Cin√©ma Club", "adresse": "23 rue des √âcoles, 75005 Paris", "type": "art_essai"},
        {"nom": "Espace Saint-Michel", "adresse": "7 place Saint-Michel, 75005 Paris", "type": "art_essai"},
        {"nom": "Le Grand Action", "adresse": "5 rue des √âcoles, 75005 Paris", "type": "art_essai"},
        {"nom": "La Filmoth√®que du Quartier Latin", "adresse": "9 rue Champollion, 75005 Paris", "type": "art_essai"},
        {"nom": "Le Desperado", "adresse": "23 rue des √âcoles, 75005 Paris", "type": "art_essai"},
        {"nom": "L'√âp√©e de Bois", "adresse": "100 rue Mouffetard, 75005 Paris", "type": "art_essai"},
        {"nom": "Le Reflet M√©dicis", "adresse": "3-5-7 rue Champollion, 75005 Paris", "type": "art_essai"},
        {"nom": "Studio des Ursulines", "adresse": "10 rue des Ursulines, 75005 Paris", "type": "art_essai"},
        {"nom": "Studio Galande", "adresse": "42 rue Galande, 75005 Paris", "type": "art_essai"},
        
        # 6e arrondissement
        {"nom": "Christine 21", "adresse": "4 rue Christine, 75006 Paris", "type": "art_essai"},
        {"nom": "Christine Cin√©ma Club", "adresse": "4 rue Christine, 75006 Paris", "type": "art_essai"},
        {"nom": "Action Christine", "adresse": "4 rue Christine, 75006 Paris", "type": "art_essai"},
        {"nom": "L'Arlequin", "adresse": "76 rue de Rennes, 75006 Paris", "type": "art_essai"},
        {"nom": "Les 3 Luxembourg", "adresse": "67 rue Monsieur-le-Prince, 75006 Paris", "type": "art_essai"},
        {"nom": "Le Lucernaire", "adresse": "53 rue Notre-Dame des Champs, 75006 Paris", "type": "art_essai"},
        {"nom": "Le Nouvel Od√©on", "adresse": "6 rue de l'√âcole de M√©decine, 75006 Paris", "type": "art_essai"},
        {"nom": "Le Saint-Andr√© des Arts", "adresse": "30 rue Saint-Andr√© des Arts, 75006 Paris", "type": "art_essai"},
        {"nom": "Le Saint-Germain-des-Pr√©s", "adresse": "22 rue Guillaume Apollinaire, 75006 Paris", "type": "art_essai"},
        {"nom": "Le Bretagne", "adresse": "73 boulevard du Montparnasse, 75006 Paris", "type": "art_essai"},
        
        # 8e-9e arrondissements
        {"nom": "Cin√©ma Katara", "adresse": "28 avenue des Champs-√âlys√©es, 75008 Paris", "type": "art_essai"},
        {"nom": "√âlys√©es Biarritz", "adresse": "22-24 rue Quentin Bauchart, 75008 Paris", "type": "art_essai"},
        {"nom": "√âlys√©es Lincoln", "adresse": "27-29 rue Lincoln, 75008 Paris", "type": "art_essai"},
        {"nom": "Le Balzac", "adresse": "1 rue Balzac, 75008 Paris", "type": "art_essai"},
        {"nom": "Le Lincoln", "adresse": "2 rue Lincoln, 75008 Paris", "type": "art_essai"},
        {"nom": "Les 5 Caumartin", "adresse": "101 rue Saint-Lazare, 75009 Paris", "type": "art_essai"},
        
        # 10e-11e arrondissements
        {"nom": "Le Louxor - Palais du Cin√©ma", "adresse": "170 boulevard de Magenta, 75010 Paris", "type": "art_essai", "url": "https://www.cinemalouxor.fr"},
        {"nom": "L'Archipel", "adresse": "17 boulevard de Strasbourg, 75010 Paris", "type": "art_essai"},
        {"nom": "Le Brady", "adresse": "39 boulevard de Strasbourg, 75010 Paris", "type": "art_essai"},
        {"nom": "Le Majestic Bastille", "adresse": "2-4 boulevard Richard Lenoir, 75011 Paris", "type": "art_essai"},
        
        # 13e-15e arrondissements
        {"nom": "L'Escurial", "adresse": "11 boulevard de Port-Royal, 75013 Paris", "type": "art_essai"},
        {"nom": "Les 7 Parnassiens", "adresse": "98 boulevard du Montparnasse, 75014 Paris", "type": "art_essai"},
        {"nom": "Chaplin Denfert", "adresse": "24 place Denfert-Rochereau, 75014 Paris", "type": "art_essai"},
        {"nom": "L'Entrep√¥t", "adresse": "7-9 rue Francis de Pressens√©, 75014 Paris", "type": "art_essai"},
        {"nom": "Chaplin Saint-Lambert", "adresse": "6 rue P√©clet, 75015 Paris", "type": "art_essai"},
        
        # 16e-20e arrondissements
        {"nom": "Le Majestic Passy", "adresse": "18 rue de Passy, 75016 Paris", "type": "art_essai"},
        {"nom": "Le Mac Mahon", "adresse": "5 avenue Mac-Mahon, 75017 Paris", "type": "art_essai"},
        {"nom": "Les 7 Batignolles", "adresse": "12 rue des Batignolles, 75017 Paris", "type": "art_essai"},
        {"nom": "Cin√©ma des Cin√©astes", "adresse": "7 avenue de Clichy, 75017 Paris", "type": "art_essai"},
        {"nom": "Club de l'√âtoile", "adresse": "14 rue Troyon, 75017 Paris", "type": "art_essai"},
        {"nom": "Studio 28", "adresse": "10 rue Tholoz√©, 75018 Paris", "type": "art_essai"},
        {"nom": "L'√âcran", "adresse": "14 passage de l'Atlas, 75019 Paris", "type": "art_essai"},
        
        # Cin√©math√®ques et institutions
        {"nom": "La G√©ode", "adresse": "26 avenue Corentin Cariou, 75019 Paris", "type": "imax"},
        {"nom": "Forum des Images", "adresse": "2 rue du Cin√©ma, Forum des Halles, 75001 Paris", "type": "cinematheque"},
        {"nom": "Cin√©math√®que Fran√ßaise", "adresse": "51 rue de Bercy, 75012 Paris", "type": "cinematheque"},
        {"nom": "Fondation J√©r√¥me Seydoux-Path√©", "adresse": "73 avenue des Gobelins, 75013 Paris", "type": "cinematheque"},
    ]
}

# Headers pour les requ√™tes
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
}


class CinemaDatabase:
    """G√®re la base de donn√©es des films et s√©ances de cin√©ma."""
    
    def __init__(self, db_path='concerts.db'):
        """Initialise la connexion √† la base de donn√©es."""
        self.db_path = db_path
        self.conn = None
        self.create_tables()
    
    def connect(self):
        """√âtablit la connexion √† la base de donn√©es."""
        self.conn = sqlite3.connect(self.db_path)
        return self.conn.cursor()
    
    def close(self):
        """Ferme la connexion √† la base de donn√©es."""
        if self.conn:
            self.conn.close()
    
    def create_tables(self):
        """Cr√©e les tables pour les cin√©mas et films."""
        cursor = self.connect()
        
        # Table des cin√©mas
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS cinemas (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                nom TEXT NOT NULL,
                adresse TEXT,
                type TEXT,
                url TEXT,
                date_ajout TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(nom, adresse)
            )
        ''')
        
        # Table des films √† l'affiche
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS films (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                url TEXT UNIQUE NOT NULL,
                nom TEXT NOT NULL,
                cinema TEXT,
                date TEXT,
                horaire TEXT,
                source TEXT,
                date_ajout TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        self.conn.commit()
        self.close()
    
    def add_cinema(self, nom, adresse, type_cinema, url=None):
        """Ajoute un cin√©ma √† la base de donn√©es."""
        cursor = self.connect()
        try:
            cursor.execute(
                'INSERT OR IGNORE INTO cinemas (nom, adresse, type, url) VALUES (?, ?, ?, ?)',
                (nom, adresse, type_cinema, url)
            )
            self.conn.commit()
            self.close()
            return True
        except Exception as e:
            print(f"Erreur ajout cin√©ma: {e}")
            self.close()
            return False
    
    def add_film(self, url, nom, cinema=None, date=None, horaire=None, source=None):
        """Ajoute un film √† la base de donn√©es."""
        cursor = self.connect()
        try:
            cursor.execute(
                'INSERT OR REPLACE INTO films (url, nom, cinema, date, horaire, source) VALUES (?, ?, ?, ?, ?, ?)',
                (url, nom, cinema, date, horaire, source)
            )
            self.conn.commit()
            self.close()
            return True
        except Exception as e:
            print(f"Erreur ajout film: {e}")
            self.close()
            return False
    
    def add_films_batch(self, films):
        """Ajoute plusieurs films en une seule transaction."""
        cursor = self.connect()
        added = 0
        for film in films:
            try:
                cursor.execute(
                    'INSERT OR REPLACE INTO films (url, nom, cinema, date, horaire, source) VALUES (?, ?, ?, ?, ?, ?)',
                    (film.get('url'), film.get('nom'), film.get('cinema'), 
                     film.get('date'), film.get('horaire'), film.get('source'))
                )
                added += 1
            except Exception as e:
                continue
        self.conn.commit()
        self.close()
        return added
    
    def get_all_cinemas(self):
        """R√©cup√®re tous les cin√©mas."""
        cursor = self.connect()
        cursor.execute('SELECT * FROM cinemas ORDER BY type, nom')
        cinemas = cursor.fetchall()
        self.close()
        return cinemas
    
    def get_all_films(self):
        """R√©cup√®re tous les films."""
        cursor = self.connect()
        cursor.execute('SELECT * FROM films ORDER BY date_ajout DESC')
        films = cursor.fetchall()
        self.close()
        return films
    
    def count_films(self):
        """Retourne le nombre total de films."""
        cursor = self.connect()
        cursor.execute('SELECT COUNT(*) FROM films')
        count = cursor.fetchone()[0]
        self.close()
        return count
    
    def count_cinemas(self):
        """Retourne le nombre total de cin√©mas."""
        cursor = self.connect()
        cursor.execute('SELECT COUNT(*) FROM cinemas')
        count = cursor.fetchone()[0]
        self.close()
        return count


def is_valid_film_title(text):
    """V√©rifie si le texte ressemble √† un titre de film valide."""
    if not text or len(text) < 3:
        return False
    
    # Exclure les textes qui ne sont clairement pas des titres de films
    exclude_patterns = [
        r'^¬©',  # Copyright
        r'^Campagne',
        r'^\d+x\d+',  # Dimensions
        r'^http',
        r'\.jpg$',
        r'\.png$',
        r'^Menu$',
        r'^Accueil$',
        r'^Voir plus$',
        r'^Lire la suite$',
        r'^En savoir plus$',
        r'^Fermer$',
        r'^Newsletter$',
        r'^Facebook$',
        r'^Twitter$',
        r'^Instagram$',
        r'^Partager$',
        r'^Connexion$',
        r'^S\'inscrire$',
        r'^Rechercher$',
        r'^Suivant$',
        r'^Pr√©c√©dent$',
    ]
    
    for pattern in exclude_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            return False
    
    # Le texte doit avoir au moins un mot d'une certaine longueur
    if not any(len(word) >= 3 for word in text.split()):
        return False
    
    return True


def clean_film_title(title):
    """Nettoie un titre de film."""
    if not title:
        return None
    
    # Supprimer les espaces multiples
    title = ' '.join(title.split())
    
    # Supprimer les caract√®res de d√©but/fin
    title = title.strip('‚Ä¢¬∑-‚Äì‚Äî ')
    
    # Limiter la longueur
    if len(title) > 150:
        title = title[:147] + "..."
    
    return title if title else None


def scrape_sortiraparis_cinema():
    """Scrape les √©v√©nements cin√©ma depuis sortiraparis.com."""
    films = []
    base_url = "https://www.sortiraparis.com"
    
    # Pages sp√©cifiques au cin√©ma
    cinema_urls = [
        f"{base_url}/loisirs/cinema/a-l-affiche.html",
        f"{base_url}/loisirs/cinema/",
        f"{base_url}/loisirs/cinema/les-films-de-la-semaine.html",
    ]
    
    for page_url in cinema_urls:
        try:
            print(f"  Scraping {page_url}...")
            response = requests.get(page_url, headers=HEADERS, timeout=30)
            
            if response.status_code != 200:
                print(f"    Erreur HTTP {response.status_code}")
                continue
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Rechercher les articles/liens de films
            # Rechercher dans les balises article, h2, h3
            for article in soup.find_all(['article', 'div'], class_=lambda x: x and ('article' in str(x).lower() or 'card' in str(x).lower() or 'item' in str(x).lower())):
                # Trouver le lien principal
                a = article.find('a', href=True)
                if a:
                    href = a.get('href', '')
                    
                    # Trouver le titre
                    title_tag = article.find(['h2', 'h3', 'h4']) or a
                    nom = title_tag.get_text(strip=True) if title_tag else None
                    
                    if not nom:
                        img = a.find('img')
                        if img:
                            nom = img.get('alt', '') or img.get('title', '')
                    
                    nom = clean_film_title(nom)
                    
                    if nom and is_valid_film_title(nom):
                        # Nettoyer et valider l'URL
                        cleaned_url = clean_url(href, base_url)
                        
                        if not cleaned_url:
                            continue
                        
                        # V√©rifier que c'est une page d'√©v√©nement (article individuel)
                        if not is_valid_event_url_pattern(cleaned_url):
                            continue
                        
                        # Doit contenir /articles/ avec un ID pour SortirAParis
                        if 'sortiraparis.com' in cleaned_url and '/articles/' not in cleaned_url:
                            continue
                        
                        films.append({
                            'url': cleaned_url,
                            'nom': nom,
                            'source': 'sortiraparis',
                            'cinema': None,
                            'date': None,
                            'horaire': None
                        })
            
            # Aussi rechercher les liens directs avec "film" ou "cinema" dans l'URL
            for a in soup.find_all('a', href=True):
                href = a.get('href', '')
                
                if any(kw in href.lower() for kw in ['/film/', '/cinema/', '/projection/']):
                    nom = a.get_text(strip=True)
                    nom = clean_film_title(nom)
                    
                    if nom and is_valid_film_title(nom) and len(nom) > 5:
                        # Nettoyer et valider l'URL
                        cleaned_url = clean_url(href, base_url)
                        
                        if not cleaned_url:
                            continue
                        
                        # V√©rifier que c'est une page d'√©v√©nement individuel
                        if not is_valid_event_url_pattern(cleaned_url):
                            continue
                        
                        # Doit contenir /articles/ avec un ID pour SortirAParis
                        if 'sortiraparis.com' in cleaned_url and '/articles/' not in cleaned_url:
                            continue
                        
                        films.append({
                            'url': cleaned_url,
                            'nom': nom,
                            'source': 'sortiraparis',
                            'cinema': None,
                            'date': None,
                            'horaire': None
                        })
            
            time.sleep(0.5)
            
        except Exception as e:
            print(f"    Erreur: {e}")
    
    # Valider et nettoyer tous les √©v√©nements
    films = validate_and_clean_events(films, base_url, verbose=False)
    
    return films


def scrape_allocine_films():
    """Scrape les films depuis allocine.fr."""
    films = []
    base_url = "https://www.allocine.fr"
    
    urls = [
        f"{base_url}/film/aucinema/",
    ]
    
    for page_url in urls:
        try:
            print(f"  Scraping {page_url}...")
            response = requests.get(page_url, headers=HEADERS, timeout=30)
            
            if response.status_code != 200:
                print(f"    Erreur HTTP {response.status_code}")
                continue
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Rechercher les liens de films (fiches individuelles)
            for a in soup.find_all('a', href=True):
                href = a.get('href', '')
                
                # Valider que c'est bien une fiche film individuelle
                if '/film/fichefilm_gen_cfilm=' in href:
                    nom = a.get_text(strip=True)
                    nom = clean_film_title(nom)
                    
                    if nom and is_valid_film_title(nom):
                        # Nettoyer et valider l'URL
                        cleaned_url = clean_url(href, base_url)
                        
                        if not cleaned_url:
                            continue
                        
                        # V√©rifier que c'est une vraie fiche film AlloCin√©
                        if not is_allocine_event_url(cleaned_url):
                            continue
                        
                        films.append({
                            'url': cleaned_url,
                            'nom': nom,
                            'source': 'allocine',
                            'cinema': None,
                            'date': None,
                            'horaire': None
                        })
            
            time.sleep(0.5)
            
        except Exception as e:
            print(f"    Erreur: {e}")
    
    # Valider et nettoyer tous les √©v√©nements
    films = validate_and_clean_events(films, base_url, verbose=False)
    
    return films


def scrape_premiere_films():
    """Scrape les films depuis premiere.fr."""
    films = []
    base_url = "https://www.premiere.fr"
    
    urls = [
        f"{base_url}/film/films-en-salles/",
        f"{base_url}/film/",
    ]
    
    for page_url in urls:
        try:
            print(f"  Scraping {page_url}...")
            response = requests.get(page_url, headers=HEADERS, timeout=30)
            
            if response.status_code != 200:
                print(f"    Erreur HTTP {response.status_code}")
                continue
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Rechercher les liens de films
            for a in soup.find_all('a', href=True):
                href = a.get('href', '')
                
                # V√©rifier que c'est une fiche film (pas une cat√©gorie)
                if '/film/' in href and href.count('/') >= 3:
                    nom = a.get_text(strip=True)
                    nom = clean_film_title(nom)
                    
                    if nom and is_valid_film_title(nom) and len(nom) > 3:
                        # Nettoyer et valider l'URL
                        cleaned_url = clean_url(href, base_url)
                        
                        if not cleaned_url:
                            continue
                        
                        # V√©rifier que c'est une page d'√©v√©nement individuel
                        if not is_valid_event_url_pattern(cleaned_url):
                            continue
                        
                        # Doit avoir un slug de film (pas juste /film/ ou /films/)
                        path_parts = cleaned_url.split('/')
                        film_idx = next((i for i, p in enumerate(path_parts) if 'film' in p.lower()), -1)
                        if film_idx == -1 or film_idx >= len(path_parts) - 1:
                            continue
                        
                        # Le slug apr√®s /film/ doit √™tre substantiel
                        film_slug = path_parts[film_idx + 1] if film_idx + 1 < len(path_parts) else ''
                        if not film_slug or len(film_slug) < 3 or film_slug in ['films-en-salles', 'a-venir', 'tous']:
                            continue
                        
                        films.append({
                            'url': cleaned_url,
                            'nom': nom,
                            'source': 'premiere',
                            'cinema': None,
                            'date': None,
                            'horaire': None
                        })
            
            time.sleep(0.5)
            
        except Exception as e:
            print(f"    Erreur: {e}")
    
    # Valider et nettoyer tous les √©v√©nements
    films = validate_and_clean_events(films, base_url, verbose=False)
    
    return films


def scrape_telerama_films():
    """Scrape les films depuis telerama.fr."""
    films = []
    base_url = "https://www.telerama.fr"
    
    urls = [
        f"{base_url}/cinema/films-a-l-affiche",
    ]
    
    for page_url in urls:
        try:
            print(f"  Scraping {page_url}...")
            response = requests.get(page_url, headers=HEADERS, timeout=30)
            
            if response.status_code != 200:
                print(f"    Erreur HTTP {response.status_code}")
                continue
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Rechercher les liens de films (fiches individuelles)
            for a in soup.find_all('a', href=True):
                href = a.get('href', '')
                
                # Les fiches films de Telerama ont le format /cinema/films/slug-film
                if '/cinema/' in href and '/films/' in href:
                    nom = a.get_text(strip=True)
                    nom = clean_film_title(nom)
                    
                    if nom and is_valid_film_title(nom):
                        # Nettoyer et valider l'URL
                        cleaned_url = clean_url(href, base_url)
                        
                        if not cleaned_url:
                            continue
                        
                        # V√©rifier que c'est une page d'√©v√©nement individuel
                        if not is_valid_event_url_pattern(cleaned_url):
                            continue
                        
                        # Doit avoir un slug de film apr√®s /films/
                        if '/films/' in cleaned_url:
                            parts = cleaned_url.split('/films/')
                            if len(parts) < 2 or not parts[1] or parts[1] in ['', 'a-l-affiche', 'prochainement']:
                                continue
                        
                        films.append({
                            'url': cleaned_url,
                            'nom': nom,
                            'source': 'telerama',
                            'cinema': None,
                            'date': None,
                            'horaire': None
                        })
            
            time.sleep(0.5)
            
        except Exception as e:
            print(f"    Erreur: {e}")
    
    # Valider et nettoyer tous les √©v√©nements
    films = validate_and_clean_events(films, base_url, verbose=False)
    
    return films


def scrape_cinema_websites():
    """Scrape les affiches directement depuis les sites des cin√©mas."""
    films = []
    
    cinema_urls = [
        ("https://www.legrandrex.com/films", "Le Grand Rex"),
        ("https://www.cinemalouxor.fr/films/a-l-affiche/", "Le Louxor"),
        ("https://www.mk2.com/films", "MK2"),
    ]
    
    for base_url, cinema_name in cinema_urls:
        try:
            print(f"  Scraping {base_url}...")
            response = requests.get(base_url, headers=HEADERS, timeout=30)
            
            if response.status_code != 200:
                print(f"    Erreur HTTP {response.status_code}")
                continue
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Rechercher les liens vers les fiches films individuelles
            for elem in soup.find_all('a', href=True):
                href = elem.get('href', '')
                nom = elem.get_text(strip=True)
                
                # Si pas de texte, chercher un titre ou h2/h3 √† l'int√©rieur
                if not nom:
                    title_elem = elem.find(['h2', 'h3', 'h4', 'span'])
                    if title_elem:
                        nom = title_elem.get_text(strip=True)
                
                nom = clean_film_title(nom)
                
                if nom and is_valid_film_title(nom) and len(nom) > 3:
                    # Nettoyer et valider l'URL
                    cleaned_url = clean_url(href, base_url)
                    
                    if not cleaned_url:
                        continue
                    
                    # V√©rifier que c'est une page de film individuel
                    # (doit avoir un slug apr√®s /films/ ou /film/)
                    if '/films/' in cleaned_url or '/film/' in cleaned_url:
                        path = cleaned_url.split('/film')[-1]
                        # Doit avoir plus qu'un simple / ou /a-l-affiche
                        if path in ['/', '/s/', '/s', ''] or 'a-l-affiche' in path:
                            continue
                        
                        # Le slug doit √™tre substantiel
                        slug = path.strip('/').split('/')[0] if path else ''
                        if not slug or len(slug) < 3:
                            continue
                    else:
                        # Pas un lien vers un film
                        continue
                    
                    films.append({
                        'url': cleaned_url,
                        'nom': nom,
                        'source': cinema_name.lower().replace(' ', '_'),
                        'cinema': cinema_name,
                        'date': None,
                        'horaire': None
                    })
            
            time.sleep(0.5)
            
        except Exception as e:
            print(f"    Erreur: {e}")
    
    return films


def remove_duplicates(films):
    """Supprime les doublons bas√©s sur l'URL et le nom."""
    seen_urls = set()
    seen_names = set()
    unique_films = []
    
    for film in films:
        url = film.get('url', '')
        nom = film.get('nom', '').lower().strip()
        
        # Nettoyer l'URL pour une meilleure d√©duplication
        cleaned_url = clean_url(url) if url else ''
        
        if not cleaned_url:
            continue
        
        # V√©rifier les doublons par URL et par nom similaire
        if cleaned_url not in seen_urls and nom not in seen_names:
            seen_urls.add(cleaned_url)
            seen_names.add(nom)
            film['url'] = cleaned_url  # Mettre √† jour avec l'URL nettoy√©e
            unique_films.append(film)
    
    return unique_films


def save_cinemas_to_db(db):
    """Enregistre tous les cin√©mas parisiens dans la base de donn√©es."""
    print("\nüìΩÔ∏è  Enregistrement des cin√©mas de Paris...")
    
    count = 0
    for cinema in PARIS_CINEMAS["reseaux"]:
        if db.add_cinema(cinema["nom"], cinema["adresse"], cinema.get("type", "reseau"), cinema.get("url")):
            count += 1
    
    for cinema in PARIS_CINEMAS["independants"]:
        if db.add_cinema(cinema["nom"], cinema["adresse"], cinema.get("type", "independant"), cinema.get("url")):
            count += 1
    
    print(f"‚úÖ {db.count_cinemas()} cin√©mas enregistr√©s")
    return count


def main():
    """Script principal pour scraper les affiches des cin√©mas de Paris."""
    print("=" * 60)
    print("üé¨ SCRAPER CIN√âMAS DE PARIS")
    print("=" * 60)
    
    # Initialiser la base de donn√©es
    db = CinemaDatabase('concerts.db')
    
    # 1. Enregistrer les cin√©mas
    save_cinemas_to_db(db)
    
    # 2. Scraper les films depuis diff√©rentes sources
    print("\nüîç Recherche des films √† l'affiche...")
    
    all_films = []
    
    # SortiraParis
    print("\nüìå Source: SortiraParis")
    films_sortiraparis = scrape_sortiraparis_cinema()
    all_films.extend(films_sortiraparis)
    print(f"   ‚Üí {len(films_sortiraparis)} liens trouv√©s")
    
    # AlloCin√©
    print("\nüìå Source: AlloCin√©")
    films_allocine = scrape_allocine_films()
    all_films.extend(films_allocine)
    print(f"   ‚Üí {len(films_allocine)} liens trouv√©s")
    
    # Premi√®re
    print("\nüìå Source: Premi√®re")
    films_premiere = scrape_premiere_films()
    all_films.extend(films_premiere)
    print(f"   ‚Üí {len(films_premiere)} liens trouv√©s")
    
    # T√©l√©rama
    print("\nüìå Source: T√©l√©rama")
    films_telerama = scrape_telerama_films()
    all_films.extend(films_telerama)
    print(f"   ‚Üí {len(films_telerama)} liens trouv√©s")
    
    # Sites de cin√©mas
    print("\nüìå Source: Sites de cin√©mas")
    films_cinemas = scrape_cinema_websites()
    all_films.extend(films_cinemas)
    print(f"   ‚Üí {len(films_cinemas)} liens trouv√©s")
    
    # Supprimer les doublons
    unique_films = remove_duplicates(all_films)
    print(f"\nüìä Total unique: {len(unique_films)} √©v√©nements cin√©ma")
    
    # Sauvegarder dans la base de donn√©es
    print("\nüíæ Sauvegarde dans la base de donn√©es...")
    added = db.add_films_batch(unique_films)
    print(f"‚úÖ {added} nouveaux films ajout√©s √† la table films")
    
    # Ajouter aussi √† la table concerts pour compatibilit√© avec l'application existante
    from database import ConcertDatabase
    concert_db = ConcertDatabase('concerts.db')
    
    concerts_to_add = [(f['url'], f['nom']) for f in unique_films if f['url'] and f['nom']]
    concerts_added = concert_db.add_concerts_batch(concerts_to_add)
    print(f"‚úÖ {concerts_added} √©v√©nements ajout√©s √† la table concerts")
    
    # Afficher les statistiques
    print("\n" + "=" * 60)
    print("üìä STATISTIQUES FINALES")
    print("=" * 60)
    print(f"üé¨ Cin√©mas de Paris: {db.count_cinemas()}")
    print(f"üéûÔ∏è  Films dans la base: {db.count_films()}")
    print(f"üéµ Total concerts/√©v√©nements: {concert_db.count_concerts()}")
    
    # Afficher quelques exemples
    print("\nüé¨ Exemples d'√©v√©nements cin√©ma enregistr√©s:")
    films = db.get_all_films()
    for film in films[:15]:
        title = film[2]
        source = film[6] if len(film) > 6 else "?"
        display = f"{title[:55]}..." if len(title) > 55 else title
        print(f"  ‚Ä¢ [{source}] {display}")
    
    # Afficher les cin√©mas par type
    print("\nüèõÔ∏è  Cin√©mas par type:")
    cinemas = db.get_all_cinemas()
    types_count = {}
    for cinema in cinemas:
        t = cinema[3] if len(cinema) > 3 else "autre"
        types_count[t] = types_count.get(t, 0) + 1
    
    for t, count in sorted(types_count.items()):
        print(f"  ‚Ä¢ {t}: {count}")
    
    print("\n‚úÖ Scraping termin√©!")
    return len(unique_films)


if __name__ == "__main__":
    main()
